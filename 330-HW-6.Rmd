---
title: "HW 6"
author: "Chris Trippe & Ethan Kemeny"
output:
  word_document: default
  html_document: default
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(message = FALSE)
```
```{r, message = FALSE, echo = FALSE}
library(tidyverse)
library(MASS)
library(lmtest)
library(bestglm)
library(GGally)
library(car)
library(ggcorrplot)
library(kableExtra)
```

1. In your own words, summarize the overarching problem and any specific questions that need to be answered
using the rating dataset. Discuss how statistical modeling will be able to answer the posed questions.

In order to know what makes a good professor, we will make a regression model by examining nine different variables (number of years the professor has been teaching, number of courses taught by professor, is the professor "hot, the professor's discipline, average easiness rating of the professor, etc.) from ratemyprofessor.com and determine which, if any, contribute to a professor's overall quality. 

2. Construct a scatterplot matrix of the data and assess any potential issues related to collinearity. Calculate
variance inflation factors and discuss what variables, if any, are collinear. Comment on what affect collinearity
can have on a regression analysis.

```{r, message = FALSE, echo = FALSE}
##Read in table
prof_data <- read.table(file ="rateprof.txt", header = TRUE)

##Data subset with quantitative variables
prof_quant <- prof_data[,sapply(prof_data,is.numeric)]

##scatter plot matrix with correlations
ggpairs(prof_quant)
```


```{r, echo = FALSE, results = "hide"}
##Linear Model with all variables
slr_prof <- lm(quality~.,prof_data)
vif_prof <- vif(slr_prof)
vif_prof <- data.frame(vif(slr_prof))
colnames(vif_prof) <- "VIF"
vif_prof <- as.matrix(cbind(Varible = row.names(vif_prof), VIF = vif_prof$VIF))

##Calculate variance inflation factors
kable(cbind(vif_prof[1:10,], vif_prof[11:20,], vif_prof[21:30,])) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

When referencing to the scatterplot matrix and associated correlation values, there doesn't appear to be any collinearity or strong relationship between two explanitory variables. The largest correlation is 0.342 between number of years and number of raters, but we'll consider than relationship insignificant enough. When calculating variance inflation factors (VIF), which works for qualitative and quantitative variables, values above ten are considered to be multicollinear. That means that variables that should be significant will not be considered significant because of their relationship with other explanitory variables. For this data set, disciplinePre, disciplineSocSci, disciplineSTEM, deptBiology, deptBusiness, and deptChemistry all have VIFs above ten. 

```{r, echo = FALSE, results = "hide"}
##Calculate variance inflation factors > 10
vif(slr_prof)[vif(slr_prof) > 10]
```

```{r}
##variance inflation factors
vif(slr_prof)
```

3. Use a variable selection technique to determine a MLR model that will answer the questions posed in #1 (do
NOT consider any interactions). Justify your choice of your selected variable selection procedure (e.g., state
why you chose to use backward instead of forward selection). Justify your choice of a model comparison
criterion (e.g. state why you chose to base your variable selection procedure on AIC vs. BIC).

```{r, echo = FALSE}
prof_data <- prof_data[,c(1:7,9:ncol(prof_data),8)]

vs.res <- bestglm(prof_data, IC = "AIC", method = "exhaustive")

slr_best <- vs.res$BestModel
```

We will be using this model to make predictions which means AIC or Akaike Information Criterion, which is less restrictive on the variables allowed in "best" model, is better for making predictions than BIC or Bayesian Informaiton Criterion. The "exhaustive" selection method was also used because it finds the best model for each length one through the total number of variables based on their AIC values, and then of those models, the best is picked. That means every possible model is tested and the one with the best AIC value is chosen.

4. Write out (in mathematical form with greek letters) your selected MLR model from #3. Clearly state any
assumptions you are using in your model. Provide an interpretation of each mathematical term (variable or
parameter) included in your model. Using the mathematical form, discuss how your model, after fitting it to
the data, will be able to answer the questions in this problem.

When using an exhaustive AIC variable selection test, the following model is built:

$y_i = \beta_0 + \beta_1numYears_i + \beta_2numCourses_i + \beta_3I(pepperyes_i = yes) + \beta_4easiness_i + \beta_5raterInterest_i + \beta_6I(disciplineSTEMYes_i = yes) + \beta_7I(deptBusinessYes_i = yes) + \beta_8I(deptEnglishYes_i = yes) + \beta_9I(deptGeologyYes_i = yes) + \beta_{10}I(deptLanguagesYes_i = yes) + \beta_{11}I(deptPhysicsYes_i = yes) + \beta{12}I(deptPolySciYes = yes) + \beta_{13}I(deptPsychologyYes_i = yes)$

By using this model, we assume that the model is linear, the values of quality are independent, the residuals are normally distributed about the regression line, and the residuals have equal variance about the line. 

$\beta_{0}$: the intercept; when all categorical variables are "no" and all quantitative variables are 0, the value of quality is $\beta_{0}$, on average.

$\beta_{1}$: Holding all other explanitory variables constant, as the number of years increases by one, quality increases by $\beta_{1}$, on average.

$\beta_{2}$: Holding all other explanitory variables constant, as the number of courses increases by one, quality increases by $\beta_{2}$, on average.

$\beta_{3}$: Holding all other explanitory variables constant, as a review changes from "no" chili pepper to "yes" chili pepper, quality increases by $\beta_{3}$, on average.

$\beta_{4}$: Holding all other explanitory variables constant, as easiness increases by one, quality increases by $\beta_{4}$, on average.

$\beta_{5}$: Holding all other explanitory variables constant, as rater interest increases by one, quality increases by $\beta_{5}$, on average.

$\beta_{6}$: Holding all other explanitory variables constant, as a review changes from "no" discipline STEM to "yes" discipline STEM, quality increases by $\beta_{6}$, on average.

$\beta_{7}$: Holding all other explanitory variables constant, as a review changes from "no" department Business to "yes" department Business, quality increases by $\beta_{7}$, on average.

$\beta_{8}$: Holding all other explanitory variables constant, as a review changes from "no" department English to "yes" department English, quality increases by $\beta_{8}$, on average.

$\beta_{9}$: Holding all other explanitory variables constant, as a review changes from "no" department Geology to "yes" department Geology, quality increases by $\beta_{9}$, on average.

$\beta_{10}$: Holding all other explanitory variables constant, as a review changes from "no" department Longuage to "yes" department Language, quality increases by $\beta_{10}$, on average.

$\beta_{11}$: Holding all other explanitory variables constant, as a review changes from "no" department Physics to "yes" department Physics, quality increases by $\beta_{11}$, on average.

$\beta_{12}$: Holding all other explanitory variables constant, as a review changes from "no" department PolySci to "yes" department PolySci, quality increases by $\beta_{12}$, on average.

$\beta_{13}$: Holding all other explanitory variables constant, as a review changes from "no" department Psychology to "yes" department Psychology, quality increases by $\beta_{13}$, on average.

In order to use this model, the estimated quality value is equal to any of the quantivative values multiplied by their beta or slope value plus the categorical values beta values when that categarical value is "yes".


5. Fit your model in #3 to the professor rating data and summarize the results by displaying the estimated coefficients
in a table with 95% confidence intervals for each parameter. Provide an example of how to interpret
1 interval for a quantitative explanatory variable and 1 interval for a categorical explanatory variable correctly
in the context of the problem.

| Variable                             | Symbol        | Estimate                      | Lower Bound                 | Upper Bound
| ------------------------------------ |-------------- | ----------------------------- | ----------------------------| -------------------------- |
| `r names(slr_best$coefficients[1])`  | $\beta_{0}$   | `r slr_best$coefficients[1]`  | `r confint(slr_best)[1,1]`  | `r confint(slr_best)[1,2]` |
| `r names(slr_best$coefficients[2])`  | $\beta_{1}$   | `r slr_best$coefficients[2]`  | `r confint(slr_best)[2,1]`  | `r confint(slr_best)[2,2]` |
| `r names(slr_best$coefficients[3])`  | $\beta_{2}$   | `r slr_best$coefficients[3]`  | `r confint(slr_best)[3,1]`  | `r confint(slr_best)[3,2]` |
| `r names(slr_best$coefficients[4])`  | $\beta_{3}$   | `r slr_best$coefficients[4]`  | `r confint(slr_best)[4,1]`  | `r confint(slr_best)[4,2]` |
| `r names(slr_best$coefficients[5])`  | $\beta_{4}$   | `r slr_best$coefficients[5]`  | `r confint(slr_best)[5,1]`  | `r confint(slr_best)[5,2]` |
| `r names(slr_best$coefficients[6])`  | $\beta_{5}$   | `r slr_best$coefficients[6]`  | `r confint(slr_best)[6,1]`  | `r confint(slr_best)[6,2]` |
| `r names(slr_best$coefficients[7])`  | $\beta_{6}$   | `r slr_best$coefficients[7]`  | `r confint(slr_best)[7,1]`  | `r confint(slr_best)[7,2]` |
| `r names(slr_best$coefficients[8])`  | $\beta_{7}$   | `r slr_best$coefficients[8]`  | `r confint(slr_best)[8,1]`  | `r confint(slr_best)[8,2]` |
| `r names(slr_best$coefficients[9])`  | $\beta_{8}$   | `r slr_best$coefficients[9]`  | `r confint(slr_best)[9,1]`  | `r confint(slr_best)[9,2]` |
| `r names(slr_best$coefficients[10])` | $\beta_{9}$   | `r slr_best$coefficients[10]` | `r confint(slr_best)[10,1]` | `r confint(slr_best)[10,2]`|
| `r names(slr_best$coefficients[11])` | $\beta_{10}$  | `r slr_best$coefficients[11]` | `r confint(slr_best)[11,1]` | `r confint(slr_best)[11,2]`|
| `r names(slr_best$coefficients[12])` | $\beta_{11}$  | `r slr_best$coefficients[12]` | `r confint(slr_best)[12,1]` | `r confint(slr_best)[12,2]`|
| `r names(slr_best$coefficients[13])` | $\beta_{12}$  | `r slr_best$coefficients[13]` | `r confint(slr_best)[13,1]` | `r confint(slr_best)[13,2]`|
| `r names(slr_best$coefficients[14])` | $\beta_{13}$  | `r slr_best$coefficients[14]` | `r confint(slr_best)[14,1]` | `r confint(slr_best)[14,2]`|

$\beta_{0}$: the intercept; when all categorical variables are "no" and all quantitative variables are 0, we are 95% confident that the value of quality is between `r confint(slr_best)[1,1]` and `r confint(slr_best)[1,2]`, on average.

$\beta_{1}$: Holding all other explanitory variables constant, as the number of years increases by one, we are 95% confident that quality increases by between `r confint(slr_best)[2,1]` and `r confint(slr_best)[2,2]`, on average.

$\beta_{3}$: Holding all other explanitory variables constant, as a review changes from "no" chili pepper to "yes" chili pepper, we are 95% confident that quality increases by between `r confint(slr_best)[4,1]` and `r confint(slr_best)[4,2]`, on average.

6. Assess the fit of your model and justify your model assumptions using appropriate graphics or summary
statistics. Provide discussion of your assessment of the model fit and assumptions on the level of your target
audience (e.g. interpret your model R2).

```{r, echo = FALSE, fig.height = 4, fig.width = 4}
#testing for linearity
avPlot(slr_best, variable = "numYears")
avPlot(slr_best, variable = "numCourses")
avPlot(slr_best, variable = "easiness")
avPlot(slr_best, variable = "raterInterest")

##testing for normality
ks.test(stdres(slr_best),"pnorm")

##testing for equal variance
bptest(slr_best)

```

By looking at the AV Plots, we can see that the model is approximately linear. A line seems to be a reasonable model for the data.

As for normality, the Kolmogorov_Smirnov test, which tests for normality, has a p-value of 0.8535, which means we fail to reject the null hypothesis and conclude that the residuals are normally distributed.

When testing for equal variance, we used the Breusch-Pagan test and obtain a value of 0.1666 which means fail to reject the null hypothesis and conculde that the residuals have equal variance. 

Lastly, we must consider independence of the values of quality each professor receives. One could argue that some students might rate more leniently than others, and if those students take a certain class form a professor in one department, they potentially will not take a the classes from a professor in the same department that teaches similar classes. This, however, and other arguments seem like a stretch and we can assume the effect of one professor's quality rating to be negligable on another professor's quality rating. Therefore, we can assume independence. 

```{r, echo = FALSE}


# Cross validate
n.cv <- 500
n.test <- .1*nrow(prof_data) #10%
bias <- rep(NA,n.cv)
rpmse <- rep(NA,n.cv)
coverage <- rep(NA,n.cv)
width <- rep(NA,n.cv)
for(cv in 1:n.cv){
  
  test.obs <- sample(nrow(prof_data),n.test)
  test.set <- prof_data[test.obs,]
  train.set <- prof_data[-test.obs,]
  
  train.lm <- lm(quality~numYears + numCourses + pepper + easiness + raterInterest + disciplineSTEM + deptBusiness + deptEnglish + deptGeology + deptLanguages + deptPhysics + deptPolySci + deptPsychology,data=train.set)
  test.preds <- predict.lm(train.lm,newdata=test.set,interval="prediction")
  
  bias[cv] <- mean(test.preds[,1]-test.set$quality)
  rpmse[cv] <- sqrt(mean((test.preds[,1]-test.set$quality)^2))
  coverage[cv] <- mean(test.set$quality > test.preds[,2] & test.set$qualit < test.preds[,3])
  width[cv] <- mean(test.preds[,3]-test.preds[,2])
}

# R-squared
r2 <- summary(slr_best)$r.squared

bias_mean <- mean(bias)
rpmse_mean <- mean(rpmse)
coverage_mean <- mean(coverage)
width_mean <- mean(width)

#quality value ranges from -3.75 to 2.433 an overal width of 6.18
```

Using a cross-validation study of 500 samples we are able to get a sense of how accurate our model is at proffesor ratings.  The bias is `r bias_mean`.  This means our model typically predicts life expectancy  `r bias_mean` above the actual value on average. That value is relatively small so we feel our model isn't very biased. The root prediction mean squared error value of `r rpmse_mean` tells us our predictions miss the mark by an average of `r rpmse_mean`. Even with the response variables being smaller values, a value of `r rpmse_mean` is about 16% of the overal range of the response variable. This is a little on the high side for margin of error, but we feel that our model is accurate enough in its predictions. 

We also wanted to look at our prediction intervals or the low and high end of our estimated values.  The study returned a coverage of `r coverage_mean` which is the percentage of prediction intervals that contain the actual value.  We have a width of `r width_mean`, we can conclude that our prediction intervals are about half the size of the response range.  This means our model will get us close to the proffesor's quality but the width is fairly large given the scale of the data.

The $R^{2}$ for this model is `r r2` this means our model of years, number of courses, pepper, easiness, rater interest, stem discipline, and the selected departments can explain `r r2 * 100`% of the variation in proffesor quality.  This value isn't great but it is considerable.  We feel that our model does an okay job at fitting the data with only about 45% of the variation in rating left unexplained

7. Provide a discussion to an incoming BYU professor about important qualities of a good professor (according
to rate my professor anway). As an example, use your model to predict (and show a corresponding 95% prediction
interval) Dr. Heaton’s quality rating score given: numYears= 5, numRaters= 54, numCourses= 2,
pepper=no (§ - at least my wife thinks I’m cute) , discipline=STEM, dept=Math, easiness= 2.5,
raterInterest= 4.2.

```{r}
# prediction data frame
dframe <- data.frame(numYears = 5, numCourses = 2, pepper = "no", easiness = 2.5, raterInterest = 4.2, disciplineSTEM = "Yes", deptBusiness = "No", deptEnglish = "No", deptGeology = "No", deptLanguages = "No", deptPhysics = "No", deptPolySci = "No", deptPsychology = "No")

#predicted value
predict_values <- predict.lm(slr_best, dframe, interval = "prediction")

```

What makes a good professor according to our data and model is their experience, if they are attractive, how easy they are, what department they are in and how interested the students who rate the proffesors are.  The number of courses seems to hurt how good a proffessor is according to their students.  THis makes sense as the teacher might be constrained time wise and in other ways due to their busyness with other classes and the students might feel a lack of availability from the proffesor.

Using our model we predict that Dr. Heaton would recieve a rating of `r predict_values[1]`.  This is positive so on average we would expect students to think he is a decent proffesor.  However, the prediction interval shows that his true rating could be as low as `r predict_values[2]` or as high as `r predict_values[3]`.  This interval tells us that his rating could be slightly negative or it could be close to the highest rating we have data of.  With this in mind, we would say that Dr. Heaton would be considered a "good" proffesor on average by his students.